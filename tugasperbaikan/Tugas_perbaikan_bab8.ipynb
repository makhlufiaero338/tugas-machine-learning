{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvNjGfS7c8pMjcbP+PsSZ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makhlufiaero338/tugas-machine-learning/blob/main/tugasperbaikan/Tugas_perbaikan_bab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llspWSawRhI2",
        "outputId": "6e16ebb3-3927-4b53-f74d-16fbfd0e7fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('movie_reviews')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDB dataset from nltk\n",
        "def load_imdb_data():\n",
        "    data = []\n",
        "    labels = []\n",
        "    for fileid in movie_reviews.fileids('pos'):\n",
        "        data.append(movie_reviews.raw(fileid))\n",
        "        labels.append('positive')\n",
        "    for fileid in movie_reviews.fileids('neg'):\n",
        "        data.append(movie_reviews.raw(fileid))\n",
        "        labels.append('negative')\n",
        "    return pd.DataFrame({\"review\": data, \"sentiment\": labels})\n",
        "\n",
        "data = load_imdb_data()\n",
        "print(f\"Dataset loaded: {data.shape[0]} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQnj8heERsYG",
        "outputId": "21e2a9b0-0d16-4cfe-983b-9280a9858228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 2000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data['review'], data['sentiment'], test_size=0.3, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "WarwnwpbRyWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Humans in the Loop\n",
        "# Let's manually validate a sample\n",
        "print(\"\\nSample for manual review:\")\n",
        "print(X_train.iloc[0][:200], \"...\\n\")\n",
        "print(f\"Label: {y_train.iloc[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxy8iUDPR1gh",
        "outputId": "da6eef6e-d4e4-4c54-ff39-d40cc24ad5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample for manual review:\n",
            "note : some may consider portions of the following text to be spoilers . \n",
            "be forewarned . \n",
            " \" all the world's a stage and all the men and women merely players they have their exits and their entrances ...\n",
            "\n",
            "Label: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From Prototype to Production\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('classifier', MultinomialNB()),\n",
        "])\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(\"\\nClassification Report (Naive Bayes):\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoiRgqc_R4Xh",
        "outputId": "67bdc9ae-c8df-4015-80da-09362775a500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report (Naive Bayes):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.84      0.81       298\n",
            "    positive       0.83      0.75      0.79       302\n",
            "\n",
            "    accuracy                           0.80       600\n",
            "   macro avg       0.80      0.80      0.80       600\n",
            "weighted avg       0.80      0.80      0.80       600\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Production Systems\n",
        "# A simple test function\n",
        "def test_pipeline(pipeline, sample_text):\n",
        "    \"\"\"Test pipeline with sample input.\"\"\"\n",
        "    try:\n",
        "        prediction = pipeline.predict([sample_text])\n",
        "        return prediction[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "\n",
        "sample_review = \"The movie was amazing, the performances were stellar, and I loved every minute of it.\"\n",
        "print(\"\\nTest Result:\", test_pipeline(pipeline, sample_review))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDVjE58cR7aR",
        "outputId": "d012ab18-0e4f-40f7-8139-ced82289c164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Result: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Your Own Estimator\n",
        "class TextLengthExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Custom transformer to extract text length.\"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([[len(text)] for text in X])\n"
      ],
      "metadata": {
        "id": "zpTO8ubFR-3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating the pipeline to include custom estimator\n",
        "pipeline_custom = Pipeline([\n",
        "    ('text_length', TextLengthExtractor()),\n",
        "    ('classifier', LogisticRegression()),\n",
        "])"
      ],
      "metadata": {
        "id": "b-GBoAvZSELK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate length-based feature\n",
        "X_train_length = TextLengthExtractor().transform(X_train)\n",
        "pipeline_custom.fit(X_train_length, y_train)\n",
        "\n",
        "print(\"\\nCustom Estimator Pipeline trained on text length.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdxCamu7SHGa",
        "outputId": "43aace10-de81-4910-80a0-2a6d9f9d38db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Estimator Pipeline trained on text length.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Networks with TensorFlow/Keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenization for NN\n",
        "max_words = 10000\n",
        "max_len = 200\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# Define the model\n",
        "nn_model = Sequential([\n",
        "    Embedding(max_words, 50, input_length=max_len),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(10, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS5LErIaSKvz",
        "outputId": "6a6cd045-497f-4632-9b7a-1b38d12a20d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "nn_model.fit(X_train_pad, (y_train == 'positive').astype(int), epochs=3, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "nn_loss, nn_accuracy = nn_model.evaluate(X_test_pad, (y_test == 'positive').astype(int))\n",
        "print(f\"\\nNeural Network Accuracy: {nn_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUMZ8aPESRoa",
        "outputId": "c8af16bc-f840-4da9-8af7-078c0994b862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.5281 - loss: 0.6925 - val_accuracy: 0.5000 - val_loss: 0.6908\n",
            "Epoch 2/3\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6813 - loss: 0.6853 - val_accuracy: 0.5893 - val_loss: 0.6850\n",
            "Epoch 3/3\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7295 - loss: 0.6668 - val_accuracy: 0.7429 - val_loss: 0.6676\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7483 - loss: 0.6632\n",
            "\n",
            "Neural Network Accuracy: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary\n",
        "print(\"\\nSummary:\")\n",
        "print(\"1. Humans in the Loop demonstrated manual review.\")\n",
        "print(\"2. Production pipeline built and tested using Naive Bayes.\")\n",
        "print(\"3. Custom estimator incorporated into a pipeline.\")\n",
        "print(\"4. Neural network model trained and evaluated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJOql9btSWd-",
        "outputId": "fa217acb-0855-4754-d9e4-3014ddb099e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary:\n",
            "1. Humans in the Loop demonstrated manual review.\n",
            "2. Production pipeline built and tested using Naive Bayes.\n",
            "3. Custom estimator incorporated into a pipeline.\n",
            "4. Neural network model trained and evaluated.\n"
          ]
        }
      ]
    }
  ]
}